{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ6PimD4rod_",
        "outputId": "6ec642a5-97a2-4b2c-93ad-c1fb54cd39b7"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Setup & Installation\n",
        "# Purpose: Install required packages, import libraries, and verify GPU availability\n",
        "# Instructions: Run this cell first. Wait for all packages to install.\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# GOURD FLOWER MULTI-CLASS CLASSIFICATION MODEL TRAINING V3\n",
        "# Multi-Class Classification: Variety + Gender + Non-Flower Rejection\n",
        "# Classes: ampalaya_bilog_female, ampalaya_bilog_male, patola_female, \n",
        "#          patola_male, not_flower\n",
        "# Compatible with Expo React Native Mobile App + Gemini AI Validation\n",
        "# ============================================================================\n",
        "\n",
        "# ‚≠ê‚≠ê‚≠ê CRITICAL: Install TensorFlow with Keras 2.x compatibility\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"üîß INSTALLING TENSORFLOW WITH KERAS 2.x COMPATIBILITY\")\n",
        "print(\"=\" * 70)\n",
        "print(\"‚ö†Ô∏è  This prevents Keras 3.x format issues with TFLite conversion\")\n",
        "print(\"‚ö†Ô∏è  DO NOT SKIP THIS STEP!\")\n",
        "print()\n",
        "\n",
        "# Set environment variable BEFORE importing TensorFlow\n",
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
        "\n",
        "# Install TensorFlow (latest version) and required packages\n",
        "print(\"Installing TensorFlow and dependencies...\")\n",
        "!pip install -q tensorflow tensorflow-hub\n",
        "!pip install -q pillow\n",
        "!pip install -q scikit-learn  # For confusion matrix\n",
        "\n",
        "print(\"\\n‚úÖ TensorFlow installed successfully!\")\n",
        "print(\"‚úÖ Keras 2.x compatibility mode enabled!\")\n",
        "print(\"‚úÖ Ready for TFLite conversion\")\n",
        "print()\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Check TensorFlow version and GPU availability\n",
        "print(\"=\" * 70)\n",
        "print(\"SYSTEM CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "# Removed Keras Version print as it causes issues with legacy Keras compatibility\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
        "\n",
        "# Verify we're using Keras 2.x compatibility mode\n",
        "tf_version = tf.__version__\n",
        "# Removed Keras version assignment as it causes issues with legacy Keras compatibility\n",
        "print(f\"\\n‚úÖ TensorFlow version: {tf_version}\")\n",
        "# print(f\"‚úÖ Keras version: {keras_version}\") # Original line, commented out\n",
        "if os.environ.get('TF_USE_LEGACY_KERAS') == '1':\n",
        "    print(\"‚úÖ Keras 2.x compatibility mode ENABLED\")\n",
        "    print(\"‚úÖ TFLite conversion will work correctly\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Keras 2.x compatibility not detected!\")\n",
        "    print(\"‚ö†Ô∏è  TFLite conversion may fail!\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚úì All libraries imported successfully!\")\n",
        "print(\"‚úì Ready to proceed to next cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tAmpsWPrxf4"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Mount Drive & Verify Dataset\n",
        "# Purpose: Connect to Google Drive and verify your dataset folders exist\n",
        "# Instructions: \n",
        "#   1. Run this cell\n",
        "#   2. Click the authorization link\n",
        "#   3. Grant access to Google Drive\n",
        "#   4. Wait for dataset verification\n",
        "# ============================================================================\n",
        "\n",
        "# Mount Google Drive to access datasets\n",
        "# force_remount=True allows you to choose which Google account to use\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CONFIGURATION - UPDATE THESE PATHS IF NEEDED\n",
        "# ============================================================================\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/EGourd/Datasets'\n",
        "\n",
        "# Define all class folders (UPDATE FOLDER NAMES TO MATCH YOUR STRUCTURE)\n",
        "CLASS_FOLDERS = {\n",
        "    'ampalaya_bilog_female': f'{DRIVE_BASE_PATH}/ampalaya_bilog_female',\n",
        "    'ampalaya_bilog_male': f'{DRIVE_BASE_PATH}/ampalaya_bilog_male',\n",
        "    'patola_female': f'{DRIVE_BASE_PATH}/patola_female',\n",
        "    'patola_male': f'{DRIVE_BASE_PATH}/patola_male',\n",
        "    'upo_smooth_female': f'{DRIVE_BASE_PATH}/upo_smooth_female',\n",
        "    'upo_smooth_male': f'{DRIVE_BASE_PATH}/upo_smooth_male',\n",
        "    'not_flower': f'{DRIVE_BASE_PATH}/not_flower',\n",
        "}\n",
        "\n",
        "# Local paths for organized data\n",
        "LOCAL_DATA_PATH = '/content/gourd_data'\n",
        "TRAIN_DIR = f'{LOCAL_DATA_PATH}/train'\n",
        "VALIDATION_DIR = f'{LOCAL_DATA_PATH}/validation'\n",
        "TEST_DIR = f'{LOCAL_DATA_PATH}/test'\n",
        "\n",
        "# Verify dataset paths\n",
        "print(\"=\" * 70)\n",
        "print(\"VERIFYING DATASET FOLDERS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_images = 0\n",
        "class_counts = {}\n",
        "\n",
        "for class_name, folder_path in CLASS_FOLDERS.items():\n",
        "    if os.path.exists(folder_path):\n",
        "        count = len([f for f in os.listdir(folder_path) \n",
        "                    if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        class_counts[class_name] = count\n",
        "        total_images += count\n",
        "        print(f\"‚úì {class_name}: {count} images\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è {class_name} not found at {folder_path}\")\n",
        "        class_counts[class_name] = 0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"Total images: {total_images}\")\n",
        "\n",
        "# Check class balance\n",
        "if total_images > 0:\n",
        "    avg_count = total_images / len(CLASS_FOLDERS)\n",
        "    print(f\"Average per class: {avg_count:.0f} images\")\n",
        "    print(\"\\nClass Balance Check:\")\n",
        "    for class_name, count in class_counts.items():\n",
        "        balance = (count / avg_count * 100) if avg_count > 0 else 0\n",
        "        status = \"‚úì\" if balance > 70 else \"‚ö†Ô∏è\"\n",
        "        print(f\"  {status} {class_name}: {balance:.1f}% of average\")\n",
        "    \n",
        "    print(\"\\nüí° Dataset Statistics:\")\n",
        "    print(f\"   ‚Ä¢ Smallest class: {min(class_counts.values())} images\")\n",
        "    print(f\"   ‚Ä¢ Largest class: {max(class_counts.values())} images\")\n",
        "    print(f\"   ‚Ä¢ Imbalance ratio: {max(class_counts.values()) / min(class_counts.values()):.2f}x\")\n",
        "    \n",
        "    if max(class_counts.values()) / min(class_counts.values()) > 2:\n",
        "        print(\"\\n‚ö†Ô∏è  WARNING: High class imbalance detected (>2x difference)\")\n",
        "        print(\"   ‚Üí Class weights will be automatically applied during training\")\n",
        "        print(\"   ‚Üí Consider data augmentation or collecting more samples for smaller classes\")\n",
        "    \n",
        "    if any(count < 300 for count in class_counts.values()):\n",
        "        print(\"\\n‚ö†Ô∏è  Warning: Some classes have <300 images. Consider collecting more data.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No images found. Please check your folder paths.\")\n",
        "\n",
        "print(\"\\n‚úì Drive mounted and dataset verified!\")\n",
        "print(\"‚úì Ready to organize dataset in next cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X2CUgNvr2WE"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Organize Dataset & Visualize Samples\n",
        "# Purpose: Split images into train/validation/test sets and preview samples\n",
        "# Instructions: \n",
        "#   1. Run this cell once (takes 2-5 minutes to copy files)\n",
        "#   2. Check the summary statistics\n",
        "#   3. Review the sample images displayed\n",
        "#   4. WARNING: Do not rerun this cell unless you want to re-split the data\n",
        "# ============================================================================\n",
        "\n",
        "def extract_and_organize_dataset():\n",
        "    \"\"\"\n",
        "    Organize multi-class dataset into train/validation/test splits\n",
        "    Split: 70% train, 15% validation, 15% test\n",
        "    \"\"\"\n",
        "    \n",
        "    # ‚≠ê CHECK IF ALREADY ORGANIZED (prevents re-splitting)\n",
        "    first_class = list(CLASS_FOLDERS.keys())[0]\n",
        "    if os.path.exists(f'{TRAIN_DIR}/{first_class}') and \\\n",
        "       len(os.listdir(f'{TRAIN_DIR}/{first_class}')) > 0:\n",
        "        print(\"‚ö†Ô∏è  Dataset already organized! Skipping to avoid re-splitting.\")\n",
        "        print(f\"   Delete {LOCAL_DATA_PATH}/ if you want to re-organize.\\n\")\n",
        "        \n",
        "        # Show existing split counts\n",
        "        print(\"Current dataset splits:\")\n",
        "        for split in ['train', 'validation', 'test']:\n",
        "            split_dir = f'{LOCAL_DATA_PATH}/{split}'\n",
        "            total = 0\n",
        "            for class_name in CLASS_FOLDERS.keys():\n",
        "                class_dir = f'{split_dir}/{class_name}'\n",
        "                if os.path.exists(class_dir):\n",
        "                    count = len(os.listdir(class_dir))\n",
        "                    total += count\n",
        "                    print(f\"   {split}/{class_name}: {count} images\")\n",
        "            print(f\"   {split.upper()} TOTAL: {total} images\\n\")\n",
        "        return\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"ORGANIZING MULTI-CLASS DATASET\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "    \n",
        "    # Create directory structure\n",
        "    for split in ['train', 'validation', 'test']:\n",
        "        for class_name in CLASS_FOLDERS.keys():\n",
        "            os.makedirs(f'{LOCAL_DATA_PATH}/{split}/{class_name}', exist_ok=True)\n",
        "    \n",
        "    # Process each class\n",
        "    split_summary = {'train': 0, 'validation': 0, 'test': 0}\n",
        "    \n",
        "    for class_name, source_dir in CLASS_FOLDERS.items():\n",
        "        if not os.path.exists(source_dir):\n",
        "            print(f\"‚ö†Ô∏è  Skipping {class_name} - folder not found\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"Processing {class_name}...\")\n",
        "        \n",
        "        # Get all image files\n",
        "        all_files = [f for f in os.listdir(source_dir) \n",
        "                    if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        \n",
        "        if len(all_files) == 0:\n",
        "            print(f\"   ‚ö†Ô∏è  No images found in {class_name}\")\n",
        "            continue\n",
        "        \n",
        "        # Shuffle for random split\n",
        "        np.random.seed(42)  # For reproducibility\n",
        "        np.random.shuffle(all_files)\n",
        "        \n",
        "        # Calculate split indices\n",
        "        total = len(all_files)\n",
        "        train_count = int(total * 0.70)\n",
        "        val_count = int(total * 0.15)\n",
        "        \n",
        "        train_files = all_files[:train_count]\n",
        "        val_files = all_files[train_count:train_count + val_count]\n",
        "        test_files = all_files[train_count + val_count:]\n",
        "        \n",
        "        # Copy files to respective directories\n",
        "        for file in train_files:\n",
        "            shutil.copy(f'{source_dir}/{file}', f'{TRAIN_DIR}/{class_name}/{file}')\n",
        "        \n",
        "        for file in val_files:\n",
        "            shutil.copy(f'{source_dir}/{file}', f'{VALIDATION_DIR}/{class_name}/{file}')\n",
        "        \n",
        "        for file in test_files:\n",
        "            shutil.copy(f'{source_dir}/{file}', f'{TEST_DIR}/{class_name}/{file}')\n",
        "        \n",
        "        # Update summary\n",
        "        split_summary['train'] += len(train_files)\n",
        "        split_summary['validation'] += len(val_files)\n",
        "        split_summary['test'] += len(test_files)\n",
        "        \n",
        "        print(f\"   ‚úì {class_name}: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DATASET ORGANIZATION COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Training set:    {split_summary['train']} images\")\n",
        "    print(f\"Validation set:  {split_summary['validation']} images\")\n",
        "    print(f\"Test set:        {split_summary['test']} images\")\n",
        "    print(f\"Total:           {sum(split_summary.values())} images\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Run dataset extraction and organization\n",
        "extract_and_organize_dataset()\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZE SAMPLE IMAGES\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_samples():\n",
        "    \"\"\"Display sample images from training set to verify data quality\"\"\"\n",
        "    print(\"Visualizing sample images from training set...\\n\")\n",
        "    \n",
        "    class_names = list(CLASS_FOLDERS.keys())\n",
        "    num_classes = len(class_names)\n",
        "    samples_per_class = 3\n",
        "    \n",
        "    fig, axes = plt.subplots(num_classes, samples_per_class, \n",
        "                            figsize=(12, num_classes * 3))\n",
        "    fig.suptitle('Sample Images from Training Set', fontsize=16, y=0.995)\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_dir = f'{TRAIN_DIR}/{class_name}'\n",
        "        \n",
        "        if not os.path.exists(class_dir):\n",
        "            continue\n",
        "        \n",
        "        images = os.listdir(class_dir)[:samples_per_class]\n",
        "        \n",
        "        for j, img_name in enumerate(images):\n",
        "            img_path = f'{class_dir}/{img_name}'\n",
        "            img = plt.imread(img_path)\n",
        "            \n",
        "            ax = axes[i, j] if num_classes > 1 else axes[j]\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            \n",
        "            if j == 0:\n",
        "                # Format class name for display\n",
        "                display_name = class_name.replace('_', ' ').title()\n",
        "                ax.set_title(display_name, fontsize=10, fontweight='bold', loc='left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_images.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úì Sample visualization saved as 'sample_images.png'\\n\")\n",
        "\n",
        "# Visualize samples\n",
        "visualize_samples()\n",
        "\n",
        "print(\"‚úì Dataset organization complete!\")\n",
        "print(\"‚úì Ready to configure model in next cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKlqrSMvsCh9"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Configure Hyperparameters & Prepare Data\n",
        "# Purpose: Set training configuration and create data generators with augmentation\n",
        "# Instructions:\n",
        "#   1. Review hyperparameters (modify if needed)\n",
        "#   2. Run this cell\n",
        "#   3. Note the number of training/validation/test samples\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETER CONFIGURATION\n",
        "# ============================================================================\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Class names (must match folder names and order)\n",
        "CLASS_NAMES = ['ampalaya_bilog_female', 'ampalaya_bilog_male', \n",
        "               'patola_female', 'patola_male', \n",
        "               'upo_smooth_female', 'upo_smooth_male',\n",
        "               'not_flower']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# Confidence threshold for \"unknown\" predictions\n",
        "CONFIDENCE_THRESHOLD = 0.65  # 65% minimum confidence\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Image Size: {IMG_HEIGHT}x{IMG_WIDTH}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Max Epochs: {EPOCHS}\")\n",
        "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"Number of Classes: {NUM_CLASSES}\")\n",
        "print(f\"Classes: {CLASS_NAMES}\")\n",
        "print(f\"Confidence Threshold: {CONFIDENCE_THRESHOLD * 100}%\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA AUGMENTATION AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "# Enhanced data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.2,\n",
        "    brightness_range=[0.7, 1.3],\n",
        "    fill_mode='nearest',\n",
        "    channel_shift_range=20\n",
        ")\n",
        "\n",
        "# Only rescaling for validation and test sets\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create data generators - CATEGORICAL for multi-class\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',  # Changed from 'binary' to 'categorical'\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',  # Changed from 'binary' to 'categorical'\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',  # Changed from 'binary' to 'categorical'\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Verify class indices\n",
        "print(\"\\nClass Indices (verify correct mapping):\")\n",
        "for class_name, idx in train_generator.class_indices.items():\n",
        "    print(f\"  {idx}: {class_name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATA GENERATORS READY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Training samples:   {train_generator.samples}\")\n",
        "print(f\"Validation samples: {validation_generator.samples}\")\n",
        "print(f\"Test samples:       {test_generator.samples}\")\n",
        "print(f\"Steps per epoch:    {train_generator.samples // BATCH_SIZE}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Calculate class weights for imbalanced datasets\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights_array))\n",
        "print(\"Class Weights (for imbalanced data):\")\n",
        "for idx, weight in class_weights.items():\n",
        "    print(f\"  {CLASS_NAMES[idx]}: {weight:.3f}\")\n",
        "\n",
        "print(\"\\n‚úì Hyperparameters configured!\")\n",
        "print(\"‚úì Data generators ready!\")\n",
        "print(\"‚úì Ready to build model in next cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko2HKYH6sEKw"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Build & Compile Model\n",
        "# Purpose: Create the MobileNetV2 model for multi-class classification\n",
        "# ============================================================================\n",
        "\n",
        "def create_multiclass_model():\n",
        "    \"\"\"\n",
        "    Create MobileNetV2 model for multi-class gourd flower classification\n",
        "    Optimized for mobile deployment with TensorFlow Lite\n",
        "    \n",
        "    Output: 7 classes with softmax activation\n",
        "    - ampalaya_bilog_female\n",
        "    - ampalaya_bilog_male\n",
        "    - patola_female\n",
        "    - patola_male\n",
        "    - upo_smooth_female\n",
        "    - upo_smooth_male\n",
        "    - not_flower\n",
        "    \"\"\"\n",
        "    \n",
        "    # MobileNetV2: Lightweight, optimized for mobile devices\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "        include_top=False,  # Remove classification layer\n",
        "        weights='imagenet'  # Use pre-trained weights\n",
        "    )\n",
        "    \n",
        "    print(\"Using MobileNetV2 (Mobile-Optimized)\")\n",
        "    \n",
        "    # Freeze base model layers initially (transfer learning phase 1)\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Build complete model with multi-class output\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.3),  # Dropout for regularization\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(NUM_CLASSES, activation='softmax')  # Multi-class with softmax\n",
        "    ])\n",
        "    \n",
        "    return model, base_model\n",
        "\n",
        "# Create model\n",
        "model, base_model = create_multiclass_model()\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()\n",
        "\n",
        "# ============================================================================\n",
        "# COMPILE MODEL FOR MULTI-CLASS\n",
        "# ============================================================================\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='categorical_crossentropy',  # Changed from 'binary_crossentropy'\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        keras.metrics.Precision(name='precision'),\n",
        "        keras.metrics.Recall(name='recall'),\n",
        "        keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"‚úì Model compiled for multi-class classification\")\n",
        "\n",
        "# ============================================================================\n",
        "# CALLBACKS FOR TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "checkpoint_dir = '/content/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=f'{checkpoint_dir}/best_model.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [early_stopping, model_checkpoint, reduce_lr]\n",
        "\n",
        "print(\"‚úì Training callbacks configured\")\n",
        "print(\"\\n‚úì Model is ready for training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YlHE2zbsHy-"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Train Model - Phase 1 (Frozen Base)\n",
        "# Purpose: Train with frozen base layers (transfer learning)\n",
        "# Expected time: 15-30 minutes\n",
        "# ============================================================================\n",
        "\n",
        "print(\"PHASE 1: Training with frozen base model\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "history_phase1 = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,  # Use class weights for imbalanced data\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úì Phase 1 training complete!\")\n",
        "print(\"‚úì Ready for Phase 2 fine-tuning in next cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg93nxUzsMil"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Fine-tune Model - Phase 2 & Evaluate\n",
        "# Purpose: Unfreeze some layers and fine-tune, then evaluate on test set\n",
        "# Expected time: 20-60 minutes\n",
        "# ============================================================================\n",
        "\n",
        "print(\"PHASE 2: Fine-tuning with unfrozen layers\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Unfreeze the last 30 layers of the base model\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE/10),\n",
        "    loss='categorical_crossentropy',  # Multi-class\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        keras.metrics.Precision(name='precision'),\n",
        "        keras.metrics.Recall(name='recall'),\n",
        "        keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Continue training\n",
        "history_phase2 = model.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    initial_epoch=history_phase1.epoch[-1],\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úì Training complete!\")\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATE MODEL ON TEST SET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATING MODEL ON TEST SET\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Load best model from checkpoint\n",
        "print(\"Loading best model from checkpoint...\")\n",
        "best_model = keras.models.load_model(f'{checkpoint_dir}/best_model.h5')\n",
        "print(\"‚úì Best model loaded\\n\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy, test_precision, test_recall, test_auc = best_model.evaluate(\n",
        "    test_generator,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Loss: {test_loss:.4f}\")\n",
        "print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"  Precision: {test_precision:.4f}\")\n",
        "print(f\"  Recall: {test_recall:.4f}\")\n",
        "print(f\"  AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
        "print(f\"  F1 Score: {f1_score:.4f}\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "print(\"\\nüìä Generating confusion matrix...\")\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Get predictions\n",
        "test_generator.reset()\n",
        "y_true = test_generator.classes\n",
        "y_pred_probs = best_model.predict(test_generator, verbose=0)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)  # Get class with highest probability\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=CLASS_NAMES,\n",
        "            yticklabels=CLASS_NAMES)\n",
        "plt.title('Multi-Class Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"‚úì Confusion matrix saved as 'confusion_matrix.png'\")\n",
        "\n",
        "print(\"\\n‚úì Evaluation complete!\")\n",
        "print(\"‚úì Ready to visualize results in next cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJIAoHxnsP0p"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Visualize & Test Predictions\n",
        "# Purpose: Plot training history and test the model on sample images\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZE TRAINING HISTORY\n",
        "# ============================================================================\n",
        "\n",
        "# Check if training history variables exist (session may have been disconnected)\n",
        "if 'history_phase1' in globals() and 'history_phase2' in globals():\n",
        "    def plot_training_history(history1, history2):\n",
        "        \"\"\"Plot training and validation metrics\"\"\"\n",
        "        \n",
        "        # Combine histories\n",
        "        acc = history1.history['accuracy'] + history2.history['accuracy']\n",
        "        val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
        "        loss = history1.history['loss'] + history2.history['loss']\n",
        "        val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
        "        \n",
        "        epochs_range = range(len(acc))\n",
        "        \n",
        "        plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        # Accuracy plot\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "        plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "        plt.axvline(x=len(history1.history['accuracy']), color='r',\n",
        "                    linestyle='--', label='Fine-tuning Start')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        \n",
        "        # Loss plot\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(epochs_range, loss, label='Training Loss')\n",
        "        plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "        plt.axvline(x=len(history1.history['loss']), color='r',\n",
        "                    linestyle='--', label='Fine-tuning Start')\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        \n",
        "        # Additional metrics\n",
        "        if 'precision' in history2.history:\n",
        "            precision = history2.history['precision']\n",
        "            recall = history2.history['recall']\n",
        "            \n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.plot(range(len(precision)), precision, label='Precision')\n",
        "            plt.plot(range(len(recall)), recall, label='Recall')\n",
        "            plt.legend(loc='lower right')\n",
        "            plt.title('Precision and Recall')\n",
        "            plt.xlabel('Epoch (Phase 2)')\n",
        "            plt.ylabel('Score')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/content/training_history.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    plot_training_history(history_phase1, history_phase2)\n",
        "    print(\"‚úì Training history plotted successfully!\\n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Note: Training history variables not found (session was likely disconnected)\")\n",
        "    print(\"   Skipping training history plot, but will continue with model testing...\")\n",
        "    print(\"   The best model is already saved in the checkpoint!\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# TEST MODEL INFERENCE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TESTING MODEL INFERENCE\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Load best model from checkpoint\n",
        "print(\"Loading best model from checkpoint...\")\n",
        "best_model = keras.models.load_model('/content/checkpoints/best_model.h5')\n",
        "print(\"‚úì Best model loaded successfully!\\n\")\n",
        "\n",
        "def predict_image(model, image_path):\n",
        "    \"\"\"Test prediction on a single image for multi-class\"\"\"\n",
        "    # Load and preprocess image\n",
        "    img = keras.preprocessing.image.load_img(\n",
        "        image_path,\n",
        "        target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
        "    )\n",
        "    img_array = keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Rescale\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    \n",
        "    # Predict\n",
        "    prediction = model.predict(img_array, verbose=0)\n",
        "    predicted_class_idx = np.argmax(prediction[0])\n",
        "    confidence = prediction[0][predicted_class_idx] * 100\n",
        "    class_name = CLASS_NAMES[predicted_class_idx]\n",
        "    \n",
        "    return class_name, confidence\n",
        "\n",
        "# Test on random test images from each class\n",
        "print(\"Sample predictions:\\n\")\n",
        "for class_name in CLASS_NAMES:\n",
        "    class_dir = f'{TEST_DIR}/{class_name}'\n",
        "    if not os.path.exists(class_dir):\n",
        "        continue\n",
        "    \n",
        "    images = os.listdir(class_dir)[:2]  # Get 2 images per class\n",
        "    \n",
        "    for img_name in images:\n",
        "        img_path = f'{class_dir}/{img_name}'\n",
        "        pred_class, confidence = predict_image(best_model, img_path)\n",
        "        emoji = \"‚úì\" if pred_class == class_name else \"‚úó\"\n",
        "        \n",
        "        # Format class names for display\n",
        "        true_display = class_name.replace('_', ' ').title()\n",
        "        pred_display = pred_class.replace('_', ' ').title()\n",
        "        \n",
        "        print(f\"{emoji} True: {true_display:25} | Predicted: {pred_display:25} ({confidence:.1f}%)\")\n",
        "\n",
        "print(\"\\n‚úì Visualization and testing complete!\")\n",
        "print(\"‚úì Ready to export models in next cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbyq0b73sTEo"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Export & Save to Google Drive\n",
        "# Purpose: Convert model to TFLite format and save all files to Google Drive\n",
        "# ============================================================================\n",
        "\n",
        "print(\"CONVERTING MODEL TO TENSORFLOW LITE FORMAT\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Load best model for export\n",
        "print(\"Loading best model for export...\")\n",
        "best_model = keras.models.load_model(f'{checkpoint_dir}/best_model.h5')\n",
        "print(\"‚úì Best model loaded for conversion\\n\")\n",
        "\n",
        "# Save full model first\n",
        "best_model.save('/content/gourd_multiclass_classifier.h5')\n",
        "print(\"‚úì Saved full model: gourd_multiclass_classifier.h5\")\n",
        "\n",
        "# Convert to TensorFlow Lite (optimized for mobile)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "\n",
        "# Optimizations for mobile deployment\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]  # Use float16 for smaller size\n",
        "\n",
        "# Convert\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save TFLite model\n",
        "tflite_path = '/content/gourd_multiclass_classifier.tflite'\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"‚úì TensorFlow Lite model saved: {tflite_path}\")\n",
        "print(f\"  Model size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE MODEL METADATA FOR MOBILE APP\n",
        "# ============================================================================\n",
        "\n",
        "# Create metadata file with model information\n",
        "metadata = {\n",
        "    \"model_name\": \"Gourd Flower Multi-Class Classifier\",\n",
        "    \"version\": \"3.0.0\",\n",
        "    \"model_type\": \"multi-class\",\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"architecture\": \"MobileNetV2\",\n",
        "    \"input_shape\": [IMG_HEIGHT, IMG_WIDTH, 3],\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"class_labels\": CLASS_NAMES,\n",
        "    \"confidence_threshold\": CONFIDENCE_THRESHOLD,\n",
        "    \"preprocessing\": {\n",
        "        \"rescale\": 1.0/255.0,\n",
        "        \"input_size\": [IMG_HEIGHT, IMG_WIDTH]\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"test_accuracy\": float(test_accuracy),\n",
        "        \"test_precision\": float(test_precision),\n",
        "        \"test_recall\": float(test_recall),\n",
        "        \"test_f1_score\": float(f1_score),\n",
        "        \"test_auc\": float(test_auc)\n",
        "    },\n",
        "    \"training_info\": {\n",
        "        \"total_epochs\": len(history_phase1.history['accuracy']) + len(history_phase2.history['accuracy']),\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"train_samples\": train_generator.samples,\n",
        "        \"val_samples\": validation_generator.samples,\n",
        "        \"test_samples\": test_generator.samples\n",
        "    },\n",
        "    \"usage_notes\": {\n",
        "        \"description\": \"Multi-class classifier for gourd flowers with non-flower rejection\",\n",
        "        \"varieties_supported\": [\"Ampalaya Bilog\", \"Patola\"],\n",
        "        \"genders_supported\": [\"Female\", \"Male\"],\n",
        "        \"rejection_class\": \"not_flower\",\n",
        "        \"integration\": \"Works with Gemini AI for validation\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save metadata\n",
        "with open('/content/model_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"‚úì Model metadata saved: model_metadata.json\")\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE ALL FILES TO GOOGLE DRIVE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAVING FILES TO GOOGLE DRIVE\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Create output directory in Google Drive\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/EGourd/Model_Versions'\n",
        "base_version_name = f'V3_MultiClass_{datetime.now().strftime(\"%m-%d-%Y\")}'\n",
        "\n",
        "# Find next available number\n",
        "counter = 1\n",
        "version_name = f'{base_version_name}_{counter}'\n",
        "output_dir = f'{MODEL_SAVE_PATH}/{version_name}'\n",
        "\n",
        "while os.path.exists(output_dir):\n",
        "    counter += 1\n",
        "    version_name = f'{base_version_name}_{counter}'\n",
        "    output_dir = f'{MODEL_SAVE_PATH}/{version_name}'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Saving to: {output_dir}\")\n",
        "print(f\"Version: {version_name}\\n\")\n",
        "\n",
        "# Copy files to Google Drive\n",
        "files_to_save = [\n",
        "    ('/content/gourd_multiclass_classifier.h5', 'Full Keras Model', '‚≠ê‚≠ê Backup/Retraining'),\n",
        "    ('/content/gourd_multiclass_classifier.tflite', 'TensorFlow Lite Model', '‚≠ê‚≠ê‚≠ê FOR EXPO APP'),\n",
        "    ('/content/model_metadata.json', 'Model Metadata', '‚≠ê‚≠ê‚≠ê FOR EXPO APP'),\n",
        "    ('/content/training_history.png', 'Training History Plot', '‚≠ê Documentation'),\n",
        "    ('/content/confusion_matrix.png', 'Confusion Matrix', '‚≠ê‚≠ê Documentation'),\n",
        "    ('/content/sample_images.png', 'Sample Images', '‚≠ê Documentation'),\n",
        "    (f'{checkpoint_dir}/best_model.h5', 'Best Model Checkpoint', '‚≠ê Backup')\n",
        "]\n",
        "\n",
        "print(\"File Purposes:\")\n",
        "print(\"‚≠ê‚≠ê‚≠ê = Essential for Expo app\")\n",
        "print(\"‚≠ê‚≠ê   = Good to keep for future training\")\n",
        "print(\"‚≠ê     = Optional documentation\\n\")\n",
        "\n",
        "for source_path, description, priority in files_to_save:\n",
        "    if os.path.exists(source_path):\n",
        "        filename = os.path.basename(source_path)\n",
        "        dest_path = f'{output_dir}/{filename}'\n",
        "        shutil.copy(source_path, dest_path)\n",
        "        size_mb = os.path.getsize(source_path) / (1024*1024)\n",
        "        print(f\"‚úì Saved: {description:30} ‚Üí {filename:35} ({size_mb:.2f} MB) {priority}\")\n",
        "    else:\n",
        "        print(f\"‚ö† Skipped: {description} (not found)\")\n",
        "\n",
        "print(f\"\\n‚úì All files saved to Google Drive\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY AND NEXT STEPS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE - V3 MULTI-CLASS MODEL SUMMARY\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"üìä Model Performance:\")\n",
        "print(f\"   ‚Ä¢ Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"   ‚Ä¢ Precision: {test_precision*100:.2f}%\")\n",
        "print(f\"   ‚Ä¢ Recall: {test_recall*100:.2f}%\")\n",
        "print(f\"   ‚Ä¢ F1 Score: {f1_score*100:.2f}%\")\n",
        "\n",
        "print(\"\\nüéØ Model Capabilities:\")\n",
        "print(f\"   ‚Ä¢ Classes: {NUM_CLASSES}\")\n",
        "print(f\"   ‚Ä¢ Varieties: Ampalaya Bilog, Patola\")\n",
        "print(f\"   ‚Ä¢ Genders: Male, Female\")\n",
        "print(f\"   ‚Ä¢ Non-flower rejection: Yes (not_flower class)\")\n",
        "print(f\"   ‚Ä¢ Confidence threshold: {CONFIDENCE_THRESHOLD*100}%\")\n",
        "\n",
        "print(\"\\nüì¶ Generated Files:\")\n",
        "print(\"   ‚Ä¢ gourd_multiclass_classifier.h5 (Full Keras model)\")\n",
        "print(\"   ‚Ä¢ gourd_multiclass_classifier.tflite ‚≠ê FOR EXPO\")\n",
        "print(\"   ‚Ä¢ model_metadata.json ‚≠ê FOR EXPO\")\n",
        "print(\"   ‚Ä¢ training_history.png, confusion_matrix.png, sample_images.png\")\n",
        "\n",
        "print(f\"\\nüìÅ Location in Google Drive:\")\n",
        "print(f\"   MyDrive/EGourd/Model_Versions/{version_name}/\")\n",
        "\n",
        "print(\"\\nüì± Next Steps for Mobile Integration:\")\n",
        "print(\"   1. Download from Google Drive:\")\n",
        "print(\"      ‚Ä¢ gourd_multiclass_classifier.tflite\")\n",
        "print(\"      ‚Ä¢ model_metadata.json\")\n",
        "print(\"   2. Place in: frontend/mobile-app/assets/models/\")\n",
        "print(\"   3. Update modelService.js to use multi-class model\")\n",
        "print(\"   4. Test with Gemini AI validation (already integrated!)\")\n",
        "\n",
        "print(\"\\nüîÑ Integration with Gemini:\")\n",
        "print(\"   ‚úì TFLite model provides fast on-device inference\")\n",
        "print(\"   ‚úì Gemini validates predictions for accuracy\")\n",
        "print(\"   ‚úì Conflict resolution when models disagree\")\n",
        "print(\"   ‚úì Confidence thresholds: 65% minimum\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ V3 MULTI-CLASS MODEL READY FOR DEPLOYMENT!\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
